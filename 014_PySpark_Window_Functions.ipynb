{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[PySpark Window Functions](https://sparkbyexamples.com/pyspark/pyspark-window-functions/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/12/16 21:34:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://vishnu:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[5]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Window Functions</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f190cdc9cc0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .master(\"local[5]\") \\\n",
    "        .appName(\"Window Functions\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+\n",
      "|employee_name|department|salary|\n",
      "+-------------+----------+------+\n",
      "|James        |Sales     |3000  |\n",
      "|Michael      |Sales     |4600  |\n",
      "|Robert       |Sales     |4100  |\n",
      "|Maria        |Finance   |3000  |\n",
      "|James        |Sales     |3000  |\n",
      "|Scott        |Finance   |3300  |\n",
      "|Jen          |Finance   |3900  |\n",
      "|Jeff         |Marketing |3000  |\n",
      "|Kumar        |Marketing |2000  |\n",
      "|Saif         |Sales     |4100  |\n",
      "+-------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "simpleData = ((\"James\", \"Sales\", 3000), \\\n",
    "    (\"Michael\", \"Sales\", 4600),  \\\n",
    "    (\"Robert\", \"Sales\", 4100),   \\\n",
    "    (\"Maria\", \"Finance\", 3000),  \\\n",
    "    (\"James\", \"Sales\", 3000),    \\\n",
    "    (\"Scott\", \"Finance\", 3300),  \\\n",
    "    (\"Jen\", \"Finance\", 3900),    \\\n",
    "    (\"Jeff\", \"Marketing\", 3000), \\\n",
    "    (\"Kumar\", \"Marketing\", 2000),\\\n",
    "    (\"Saif\", \"Sales\", 4100) \\\n",
    "  )\n",
    " \n",
    "columns= [\"employee_name\", \"department\", \"salary\"]\n",
    "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"DF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the window\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "windowSpec = Window.partitionBy(\"department\").orderBy(\"salary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----------+\n",
      "|employee_name|department|salary|row number|\n",
      "+-------------+----------+------+----------+\n",
      "|Maria        |Finance   |3000  |1         |\n",
      "|Scott        |Finance   |3300  |2         |\n",
      "|Jen          |Finance   |3900  |3         |\n",
      "|Kumar        |Marketing |2000  |1         |\n",
      "|Jeff         |Marketing |3000  |2         |\n",
      "|James        |Sales     |3000  |1         |\n",
      "|James        |Sales     |3000  |2         |\n",
      "|Robert       |Sales     |4100  |3         |\n",
      "|Saif         |Sales     |4100  |4         |\n",
      "|Michael      |Sales     |4600  |5         |\n",
      "+-------------+----------+------+----------+\n",
      "\n",
      "+-------------+----------+------+-------+\n",
      "|employee_name|department|salary|ROW_NUM|\n",
      "+-------------+----------+------+-------+\n",
      "|        Maria|   Finance|  3000|      1|\n",
      "|        Scott|   Finance|  3300|      2|\n",
      "|          Jen|   Finance|  3900|      3|\n",
      "|        Kumar| Marketing|  2000|      1|\n",
      "|         Jeff| Marketing|  3000|      2|\n",
      "|        James|     Sales|  3000|      1|\n",
      "|        James|     Sales|  3000|      2|\n",
      "|       Robert|     Sales|  4100|      3|\n",
      "|         Saif|     Sales|  4100|      4|\n",
      "|      Michael|     Sales|  4600|      5|\n",
      "+-------------+----------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions\n",
    "# row number() - return a number from 1 within the partition\n",
    "\n",
    "df.withColumn(\"row number\", functions.row_number().over(windowSpec)) \\\n",
    ".show(truncate=False)\n",
    "\n",
    "spark.sql('''\n",
    "        SELECT employee_name, department, \n",
    "          salary, ROW_NUMBER() OVER(PARTITION BY department ORDER BY salary) AS ROW_NUM\n",
    "        FROM DF\n",
    "        ''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----+\n",
      "|employee_name|department|salary|rank|\n",
      "+-------------+----------+------+----+\n",
      "|        Maria|   Finance|  3000|   1|\n",
      "|        Scott|   Finance|  3300|   2|\n",
      "|          Jen|   Finance|  3900|   3|\n",
      "|        Kumar| Marketing|  2000|   1|\n",
      "|         Jeff| Marketing|  3000|   2|\n",
      "|        James|     Sales|  3000|   1|\n",
      "|        James|     Sales|  3000|   1|\n",
      "|       Robert|     Sales|  4100|   3|\n",
      "|         Saif|     Sales|  4100|   3|\n",
      "|      Michael|     Sales|  4600|   5|\n",
      "+-------------+----------+------+----+\n",
      "\n",
      "+-------------+----------+------+----+\n",
      "|employee_name|department|salary|Rank|\n",
      "+-------------+----------+------+----+\n",
      "|        Maria|   Finance|  3000|   1|\n",
      "|        Scott|   Finance|  3300|   2|\n",
      "|          Jen|   Finance|  3900|   3|\n",
      "|        Kumar| Marketing|  2000|   1|\n",
      "|         Jeff| Marketing|  3000|   2|\n",
      "|        James|     Sales|  3000|   1|\n",
      "|        James|     Sales|  3000|   1|\n",
      "|       Robert|     Sales|  4100|   3|\n",
      "|         Saif|     Sales|  4100|   3|\n",
      "|      Michael|     Sales|  4600|   5|\n",
      "+-------------+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# rank() - ranks the result within a partition\n",
    "df.withColumn(\"rank\", functions.rank().over(windowSpec)) \\\n",
    "    .show()\n",
    "\n",
    "spark.sql('''\n",
    "        SELECT employee_name, department, \n",
    "          salary, RANK() OVER(PARTITION BY department ORDER BY salary) AS Rank\n",
    "        FROM DF\n",
    "        ''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----------+\n",
      "|employee_name|department|salary|dense rank|\n",
      "+-------------+----------+------+----------+\n",
      "|        Maria|   Finance|  3000|         1|\n",
      "|        Scott|   Finance|  3300|         2|\n",
      "|          Jen|   Finance|  3900|         3|\n",
      "|        Kumar| Marketing|  2000|         1|\n",
      "|         Jeff| Marketing|  3000|         2|\n",
      "|        James|     Sales|  3000|         1|\n",
      "|        James|     Sales|  3000|         1|\n",
      "|       Robert|     Sales|  4100|         2|\n",
      "|         Saif|     Sales|  4100|         2|\n",
      "|      Michael|     Sales|  4600|         3|\n",
      "+-------------+----------+------+----------+\n",
      "\n",
      "+-------------+----------+------+----------+\n",
      "|employee_name|department|salary|Dense_Rank|\n",
      "+-------------+----------+------+----------+\n",
      "|        Maria|   Finance|  3000|         1|\n",
      "|        Scott|   Finance|  3300|         2|\n",
      "|          Jen|   Finance|  3900|         3|\n",
      "|        Kumar| Marketing|  2000|         1|\n",
      "|         Jeff| Marketing|  3000|         2|\n",
      "|        James|     Sales|  3000|         1|\n",
      "|        James|     Sales|  3000|         1|\n",
      "|       Robert|     Sales|  4100|         2|\n",
      "|         Saif|     Sales|  4100|         2|\n",
      "|      Michael|     Sales|  4600|         3|\n",
      "+-------------+----------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dense_rank() - ranks without gaps\n",
    "df.withColumn(\"dense rank\", functions.dense_rank().over(windowSpec)) \\\n",
    "    .show()\n",
    "\n",
    "spark.sql('''\n",
    "        SELECT employee_name, department, \n",
    "          salary, DENSE_RANK() OVER(PARTITION BY department ORDER BY salary) AS Dense_Rank\n",
    "        FROM DF\n",
    "        ''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+------------+\n",
      "|employee_name|department|salary|percent rank|\n",
      "+-------------+----------+------+------------+\n",
      "|        Maria|   Finance|  3000|         0.0|\n",
      "|        Scott|   Finance|  3300|         0.5|\n",
      "|          Jen|   Finance|  3900|         1.0|\n",
      "|        Kumar| Marketing|  2000|         0.0|\n",
      "|         Jeff| Marketing|  3000|         1.0|\n",
      "|        James|     Sales|  3000|         0.0|\n",
      "|        James|     Sales|  3000|         0.0|\n",
      "|       Robert|     Sales|  4100|         0.5|\n",
      "|         Saif|     Sales|  4100|         0.5|\n",
      "|      Michael|     Sales|  4600|         1.0|\n",
      "+-------------+----------+------+------------+\n",
      "\n",
      "+-------------+----------+------+------------+\n",
      "|employee_name|department|salary|Percent_Rank|\n",
      "+-------------+----------+------+------------+\n",
      "|        Maria|   Finance|  3000|         0.0|\n",
      "|        Scott|   Finance|  3300|         0.5|\n",
      "|          Jen|   Finance|  3900|         1.0|\n",
      "|        Kumar| Marketing|  2000|         0.0|\n",
      "|         Jeff| Marketing|  3000|         1.0|\n",
      "|        James|     Sales|  3000|         0.0|\n",
      "|        James|     Sales|  3000|         0.0|\n",
      "|       Robert|     Sales|  4100|         0.5|\n",
      "|         Saif|     Sales|  4100|         0.5|\n",
      "|      Michael|     Sales|  4600|         1.0|\n",
      "+-------------+----------+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# percent_rank() - returns the relative rank over a partition\n",
    "df.withColumn(\"percent rank\", functions.percent_rank().over(windowSpec)) \\\n",
    "    .show()\n",
    "\n",
    "spark.sql('''\n",
    "        SELECT employee_name, department, \n",
    "          salary, PERCENT_RANK() OVER(PARTITION BY department ORDER BY salary) AS Percent_Rank\n",
    "        FROM DF\n",
    "        ''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+-----+\n",
      "|employee_name|department|salary|Ntile|\n",
      "+-------------+----------+------+-----+\n",
      "|        Maria|   Finance|  3000|    1|\n",
      "|        Scott|   Finance|  3300|    1|\n",
      "|          Jen|   Finance|  3900|    2|\n",
      "|        Kumar| Marketing|  2000|    1|\n",
      "|         Jeff| Marketing|  3000|    2|\n",
      "|        James|     Sales|  3000|    1|\n",
      "|        James|     Sales|  3000|    1|\n",
      "|       Robert|     Sales|  4100|    1|\n",
      "|         Saif|     Sales|  4100|    2|\n",
      "|      Michael|     Sales|  4600|    2|\n",
      "+-------------+----------+------+-----+\n",
      "\n",
      "+-------------+----------+------+-----+\n",
      "|employee_name|department|salary|Ntile|\n",
      "+-------------+----------+------+-----+\n",
      "|        Maria|   Finance|  3000|    1|\n",
      "|        Scott|   Finance|  3300|    1|\n",
      "|          Jen|   Finance|  3900|    1|\n",
      "|        Kumar| Marketing|  2000|    1|\n",
      "|         Jeff| Marketing|  3000|    1|\n",
      "|        James|     Sales|  3000|    1|\n",
      "|        James|     Sales|  3000|    1|\n",
      "|       Robert|     Sales|  4100|    1|\n",
      "|         Saif|     Sales|  4100|    1|\n",
      "|      Michael|     Sales|  4600|    1|\n",
      "+-------------+----------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ntile(2) - first 50 percentile data get 1 next 50 percentile data get 2\n",
    "df.withColumn(\"Ntile\", functions.ntile(2).over(windowSpec)) \\\n",
    "    .show()\n",
    "\n",
    "spark.sql('''\n",
    "        SELECT employee_name, department, \n",
    "          salary, NTILE() OVER(PARTITION BY department ORDER BY salary) AS Ntile\n",
    "        FROM DF\n",
    "        ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analytical Functions\n",
    "- cume_dist()\n",
    "- lag()\n",
    "- lead()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+-----------------------+\n",
      "|employee_name|department|salary|cumulative Distribution|\n",
      "+-------------+----------+------+-----------------------+\n",
      "|        Maria|   Finance|  3000|     0.3333333333333333|\n",
      "|        Scott|   Finance|  3300|     0.6666666666666666|\n",
      "|          Jen|   Finance|  3900|                    1.0|\n",
      "|        Kumar| Marketing|  2000|                    0.5|\n",
      "|         Jeff| Marketing|  3000|                    1.0|\n",
      "|        James|     Sales|  3000|                    0.4|\n",
      "|        James|     Sales|  3000|                    0.4|\n",
      "|       Robert|     Sales|  4100|                    0.8|\n",
      "|         Saif|     Sales|  4100|                    0.8|\n",
      "|      Michael|     Sales|  4600|                    1.0|\n",
      "+-------------+----------+------+-----------------------+\n",
      "\n",
      "+-------------+----------+------+------------------+\n",
      "|employee_name|department|salary|          Cum_dist|\n",
      "+-------------+----------+------+------------------+\n",
      "|        Maria|   Finance|  3000|0.3333333333333333|\n",
      "|        Scott|   Finance|  3300|0.6666666666666666|\n",
      "|          Jen|   Finance|  3900|               1.0|\n",
      "|        Kumar| Marketing|  2000|               0.5|\n",
      "|         Jeff| Marketing|  3000|               1.0|\n",
      "|        James|     Sales|  3000|               0.4|\n",
      "|        James|     Sales|  3000|               0.4|\n",
      "|       Robert|     Sales|  4100|               0.8|\n",
      "|         Saif|     Sales|  4100|               0.8|\n",
      "|      Michael|     Sales|  4600|               1.0|\n",
      "+-------------+----------+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# cume_dist() - get the cumulative distribution of values within a partition\n",
    "df.withColumn(\"cumulative Distribution\", functions.cume_dist() \\\n",
    "              .over(windowSpec)).show()\n",
    "# like dense_rank()\n",
    "\n",
    "spark.sql('''\n",
    "        SELECT employee_name, department, \n",
    "          salary, CUME_DIST() OVER(PARTITION BY department ORDER BY salary) AS Cum_dist\n",
    "        FROM DF\n",
    "        ''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----+\n",
      "|employee_name|department|salary| Lag|\n",
      "+-------------+----------+------+----+\n",
      "|        Maria|   Finance|  3000|NULL|\n",
      "|        Scott|   Finance|  3300|NULL|\n",
      "|          Jen|   Finance|  3900|3000|\n",
      "|        Kumar| Marketing|  2000|NULL|\n",
      "|         Jeff| Marketing|  3000|NULL|\n",
      "|        James|     Sales|  3000|NULL|\n",
      "|        James|     Sales|  3000|NULL|\n",
      "|       Robert|     Sales|  4100|3000|\n",
      "|         Saif|     Sales|  4100|3000|\n",
      "|      Michael|     Sales|  4600|4100|\n",
      "+-------------+----------+------+----+\n",
      "\n",
      "+-------------+----------+------+----+\n",
      "|employee_name|department|salary| Lag|\n",
      "+-------------+----------+------+----+\n",
      "|        Maria|   Finance|  3000|NULL|\n",
      "|        Scott|   Finance|  3300|NULL|\n",
      "|          Jen|   Finance|  3900|3000|\n",
      "|        Kumar| Marketing|  2000|NULL|\n",
      "|         Jeff| Marketing|  3000|NULL|\n",
      "|        James|     Sales|  3000|NULL|\n",
      "|        James|     Sales|  3000|NULL|\n",
      "|       Robert|     Sales|  4100|3000|\n",
      "|         Saif|     Sales|  4100|3000|\n",
      "|      Michael|     Sales|  4600|4100|\n",
      "+-------------+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lag()\n",
    "df.withColumn(\"Lag\", functions.lag(\"salary\", 2).over(windowSpec)).show()\n",
    "\n",
    "spark.sql('''\n",
    "        SELECT employee_name, department, \n",
    "          salary, LAG(salary, 2) OVER(PARTITION BY department ORDER BY salary) AS Lag\n",
    "        FROM DF\n",
    "        ''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----+\n",
      "|employee_name|department|salary|Lead|\n",
      "+-------------+----------+------+----+\n",
      "|        Maria|   Finance|  3000|3900|\n",
      "|        Scott|   Finance|  3300|NULL|\n",
      "|          Jen|   Finance|  3900|NULL|\n",
      "|        Kumar| Marketing|  2000|NULL|\n",
      "|         Jeff| Marketing|  3000|NULL|\n",
      "|        James|     Sales|  3000|4100|\n",
      "|        James|     Sales|  3000|4100|\n",
      "|       Robert|     Sales|  4100|4600|\n",
      "|         Saif|     Sales|  4100|NULL|\n",
      "|      Michael|     Sales|  4600|NULL|\n",
      "+-------------+----------+------+----+\n",
      "\n",
      "+-------------+----------+------+----+\n",
      "|employee_name|department|salary|Lead|\n",
      "+-------------+----------+------+----+\n",
      "|        Maria|   Finance|  3000|3900|\n",
      "|        Scott|   Finance|  3300|NULL|\n",
      "|          Jen|   Finance|  3900|NULL|\n",
      "|        Kumar| Marketing|  2000|NULL|\n",
      "|         Jeff| Marketing|  3000|NULL|\n",
      "|        James|     Sales|  3000|4100|\n",
      "|        James|     Sales|  3000|4100|\n",
      "|       Robert|     Sales|  4100|4600|\n",
      "|         Saif|     Sales|  4100|NULL|\n",
      "|      Michael|     Sales|  4600|NULL|\n",
      "+-------------+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lead()\n",
    "df.withColumn(\"Lead\", functions.lead(\"salary\", 2).over(windowSpec)).show()\n",
    "\n",
    "spark.sql('''\n",
    "        SELECT employee_name, department, \n",
    "          salary, LEAD(salary, 2) OVER(PARTITION BY department ORDER BY salary) AS Lead\n",
    "        FROM DF\n",
    "        ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-----+----+----+\n",
      "|department|   AVG|  SUM| MIN| MAX|\n",
      "+----------+------+-----+----+----+\n",
      "|   Finance|3400.0|10200|3000|3900|\n",
      "| Marketing|2500.0| 5000|2000|3000|\n",
      "|     Sales|3760.0|18800|3000|4600|\n",
      "+----------+------+-----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "windowSpecAgg = Window.partitionBy(\"department\")\n",
    "df.withColumn(\"row_num\", functions.row_number().over(windowSpec)) \\\n",
    "    .withColumn(\"AVG\", functions.avg(\"salary\").over(windowSpecAgg)) \\\n",
    "    .withColumn(\"SUM\", functions.sum(\"salary\").over(windowSpecAgg)) \\\n",
    "    .withColumn(\"MIN\", functions.min(\"salary\").over(windowSpecAgg)) \\\n",
    "    .withColumn(\"MAX\", functions.max(\"salary\").over(windowSpecAgg)) \\\n",
    "    .where(\"row_num == 1\") \\\n",
    "    .select(\"department\", \"AVG\", \"SUM\", \"MIN\", \"MAX\") \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-----+----+----+\n",
      "|department|   avg|  sum| min| max|\n",
      "+----------+------+-----+----+----+\n",
      "|   Finance|3400.0|10200|3000|3900|\n",
      "| Marketing|2500.0| 5000|2000|3000|\n",
      "|     Sales|3760.0|18800|3000|4600|\n",
      "+----------+------+-----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "        SELECT department, avg, sum, min, max\n",
    "          FROM (\n",
    "          SELECT department,\n",
    "          ROW_NUMBER() OVER(PARTITION BY department ORDER BY salary) as row,\n",
    "          AVG(salary) OVER(PARTITION BY department) as avg,\n",
    "          SUM(salary) OVER(PARTITION BY department) as sum,\n",
    "          MIN(salary) OVER(PARTITION BY department) as min,\n",
    "          MAX(salary) OVER(PARTITION BY department) as max\n",
    "          FROM DF) as temp\n",
    "          WHERE temp.row = 1\n",
    "        ''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySpark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
